# $train_in and $train_out specify whether we train on in-domain data
# or out-of-domain data (all training sets except the in-domain
# training set), or both at the same time. At least one of the two
# must be set to 1.
train_in=1
train_out=0

# $dev_in and $dev_out specify whether we validate on in-domain data
# or out-of-domain data (all dev sets except the in-domain dev set),
# or both at the same time. At least one of the two must be set to 1.
dev_in=1
dev_out=0

# Should we map the entity types of the datasets to a common set
# (i.e. PER, ORG, LOC, MISC)? If either train_out=1 or dev_out=1, then
# map should be set to 1.
map=1

# Should we down-sample negative examples by discarding sentences that
# contain no mentions of named entities from the training set?
down=0

# Set space-separated list of names of datasets used for testing and
# for in-domain training and/or validation.
#
# For each of these names, $dir_data must contain a file matching the
# pattern <name>.test.iob.
#
# If $train_in=1, $dir_data must contain a file matching the pattern
# <name>.train.iob for each of these names.
#
# If $dev_out=0, $dir_data must contain a file matching the pattern
# <name>.dev.iob for each of these names.
test_dnames="conll ontonotes"

# Set space-separated list of names of datasets used for out-of-domain
# training and/or validation.
#
# This list is ignored if $train_out=0 and $dev_out=0.
#
# If $train_out=1, $dir_data must contain a file matching the pattern
# <name>.train.iob for each of these names.
#
# If $dev_out=1, $dir_data must contain a file matching the pattern
# <name>.dev.iob for each of these names.
train_dnames="conll ontonotes"

# Set directory containing datasets. It must contain files matching
# the patterns <name>.<[train|dev|test]>.iob for each of the names in
# $test_dnames and optionally $train_dnames (see comments on those 2
# settings).
dir_data=/path/to/datasets

# Set path of ner_eval
dir_ner_eval=/path/to/ner_eval

# Set directory where we can temporarily write stuff
dir_tmp=/path/to/tmp

# CRF++ template file
path_crfpp_templates=$dir_ner_eval/exp/crfpp_templates/example-tokens-only.txt

# Directory containing Stanford NER jar
dir_stanford_ner=/path/to/stanford-ner-2018-02-27

# Config file for Stanford NER which we will use as a template. This
# template must be a valid config used to train a model.
path_stanford_config_template=/path/to/stanford-ner-prop-file

# Max number of gb of memory that we can use (only used for Stanford
# NER at the moment)
max_mem_gb=16

# Path of NER package within CogComp-NLP
dir_illinois_ner=/path/to/cogcomp-nlp-4.0.9/ner

# Path of NeuroNER
dir_neuroner=/path/to/NeuroNER-master

# Path of pre-trained word embeddings (in GloVe's text format) to use
# with NeuroNER.
path_embeddings=/path/to/glove-embeddings

# Path of Spacy model containing vocab and word embeddings.
path_spacy_embed_model=/path/to/spacy-word-embedding-model

# Which GPU do we use with SpaCy? (-1 to use CPU only)
spacy_gpu=0
